{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minimal_idcm_usage_example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python371jvsc74a57bd0c850e6f77ff7c9cbece5364f8526ec42dd183cf59251b1cfd7b71a0467b242c1",
      "display_name": "Python 3.7.1 64-bit ('deep_learning': conda)"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng63tDwZSSm5"
      },
      "source": [
        "# Using Our IDCM Checkpoint\n",
        "\n",
        "We provide a fully retrieval trained DistilBert-based instance of the IDCM model on the HuggingFace model hub here: https://huggingface.co/sebastian-hofstaetter/idcm-distilbert-msmarco_doc\n",
        "\n",
        "This instance can be used to **re-rank a candidate set** of long documents. \n",
        "\n",
        "If you want to know more about our intra document cascading, check out our paper: https://arxiv.org/abs/2105.09816 üéâ\n",
        "\n",
        "This notebook gives you a minimal usage example of downloading our IDCM checkpoint to encode documents and queries to create a score of their relevance. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let's get started by installing the awesome *transformers* library from HuggingFace:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2WyNOE2R2rW"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkWDa_jWu7c"
      },
      "source": [
        "The next step is to download our checkpoint and initialize the tokenizer and models:\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTYEtziISSDl"
      },
      "source": [
        "from typing import Dict, Union\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from transformers import PreTrainedModel,PretrainedConfig\n",
        "\n",
        "pre_trained_model_name = \"sebastian-hofstaetter/idcm-distilbert-msmarco_doc\"\n",
        "\n",
        "class IDCM_Config(PretrainedConfig):\n",
        "    bert_model:str\n",
        "    # how many passages get scored by BERT \n",
        "    sample_n:int\n",
        "\n",
        "    # type of fast module\n",
        "    sample_context:str\n",
        "\n",
        "    # how many passages to take from bert to create the final score (usually the same as sample_n, but could be set to 1 for max-p)\n",
        "    top_k_chunks:int\n",
        "\n",
        "    # window size\n",
        "    chunk_size:int\n",
        "\n",
        "    # left and right overlap (added to each window)\n",
        "    overlap:int \n",
        "\n",
        "    padding_idx:int = 0\n",
        "\n",
        "class IDCM_InferenceOnly(PreTrainedModel):\n",
        "    '''\n",
        "    IDCM is a neural re-ranking model for long documents, it creates an intra-document cascade between a fast (CK) and a slow module (BERT_Cat)\n",
        "    This code is only usable for inference (we removed the training mechanism for simplicity)\n",
        "    '''\n",
        "\n",
        "    config_class = IDCM_Config\n",
        "    base_model_prefix = \"bert_model\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 cfg) -> None:\n",
        "        super().__init__(cfg)\n",
        "\n",
        "        #\n",
        "        # bert - scoring\n",
        "        #\n",
        "        if isinstance(cfg.bert_model, str):\n",
        "            self.bert_model = AutoModel.from_pretrained(cfg.bert_model)\n",
        "        else:\n",
        "            self.bert_model = cfg.bert_model\n",
        "\n",
        "        #\n",
        "        # final scoring (combination of bert scores)\n",
        "        #\n",
        "        self._classification_layer = torch.nn.Linear(self.bert_model.config.hidden_size, 1)\n",
        "        self.top_k_chunks = cfg.top_k_chunks\n",
        "        self.top_k_scoring = nn.Parameter(torch.full([1,self.top_k_chunks], 1, dtype=torch.float32, requires_grad=True))\n",
        "\n",
        "        #\n",
        "        # local self attention\n",
        "        #\n",
        "        self.padding_idx= cfg.padding_idx\n",
        "        self.chunk_size = cfg.chunk_size\n",
        "        self.overlap = cfg.overlap\n",
        "        self.extended_chunk_size = self.chunk_size + 2 * self.overlap\n",
        "\n",
        "        #\n",
        "        # sampling stuff\n",
        "        #\n",
        "        self.sample_n = cfg.sample_n\n",
        "        self.sample_context = cfg.sample_context\n",
        "\n",
        "        if self.sample_context == \"ck\":\n",
        "            i = 3\n",
        "            self.sample_cnn3 = nn.Sequential(\n",
        "                        nn.ConstantPad1d((0,i - 1), 0),\n",
        "                        nn.Conv1d(kernel_size=i, in_channels=self.bert_model.config.dim, out_channels=self.bert_model.config.dim),\n",
        "                        nn.ReLU()\n",
        "                        ) \n",
        "        elif self.sample_context == \"ck-small\":\n",
        "            i = 3\n",
        "            self.sample_projector = nn.Linear(self.bert_model.config.dim,384)\n",
        "            self.sample_cnn3 = nn.Sequential(\n",
        "                        nn.ConstantPad1d((0,i - 1), 0),\n",
        "                        nn.Conv1d(kernel_size=i, in_channels=384, out_channels=128),\n",
        "                        nn.ReLU()\n",
        "                        ) \n",
        "\n",
        "        self.sampling_binweights = nn.Linear(11, 1, bias=True)\n",
        "        torch.nn.init.uniform_(self.sampling_binweights.weight, -0.01, 0.01)\n",
        "        self.kernel_alpha_scaler = nn.Parameter(torch.full([1,1,11], 1, dtype=torch.float32, requires_grad=True))\n",
        "\n",
        "        self.register_buffer(\"mu\",nn.Parameter(torch.tensor([1.0, 0.9, 0.7, 0.5, 0.3, 0.1, -0.1, -0.3, -0.5, -0.7, -0.9]), requires_grad=False).view(1, 1, 1, -1))\n",
        "        self.register_buffer(\"sigma\", nn.Parameter(torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]), requires_grad=False).view(1, 1, 1, -1))\n",
        "        \n",
        "\n",
        "    def forward(self,\n",
        "                query: Dict[str, torch.LongTensor],\n",
        "                document: Dict[str, torch.LongTensor],\n",
        "                use_fp16:bool = True,\n",
        "                output_secondary_output: bool = False):\n",
        "\n",
        "        #\n",
        "        # patch up documents - local self attention\n",
        "        #\n",
        "        document_ids = document[\"input_ids\"][:,1:]\n",
        "        if document_ids.shape[1] > self.overlap:\n",
        "            needed_padding = self.extended_chunk_size - (((document_ids.shape[1]) % self.chunk_size)  - self.overlap)\n",
        "        else:\n",
        "            needed_padding = self.extended_chunk_size - self.overlap - document_ids.shape[1]\n",
        "        orig_doc_len = document_ids.shape[1]\n",
        "\n",
        "        document_ids = nn.functional.pad(document_ids,(self.overlap, needed_padding),value=self.padding_idx)\n",
        "        chunked_ids = document_ids.unfold(1,self.extended_chunk_size,self.chunk_size)\n",
        "\n",
        "        batch_size = chunked_ids.shape[0]\n",
        "        chunk_pieces = chunked_ids.shape[1]\n",
        "\n",
        "\n",
        "        chunked_ids_unrolled=chunked_ids.reshape(-1,self.extended_chunk_size)\n",
        "        packed_indices = (chunked_ids_unrolled[:,self.overlap:-self.overlap] != self.padding_idx).any(-1)\n",
        "        orig_packed_indices = packed_indices.clone()\n",
        "        ids_packed = chunked_ids_unrolled[packed_indices]\n",
        "        mask_packed = (ids_packed != self.padding_idx)\n",
        "\n",
        "        total_chunks=chunked_ids_unrolled.shape[0]\n",
        "\n",
        "        packed_query_ids = query[\"input_ids\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"input_ids\"].shape[1])[packed_indices]\n",
        "        packed_query_mask = query[\"attention_mask\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"attention_mask\"].shape[1])[packed_indices]\n",
        "\n",
        "        #\n",
        "        # sampling\n",
        "        # \n",
        "        if self.sample_n > -1:\n",
        "            \n",
        "            #\n",
        "            # ck learned matches\n",
        "            #\n",
        "            if self.sample_context == \"ck-small\":\n",
        "                query_ctx = torch.nn.functional.normalize(self.sample_cnn3(self.sample_projector(self.bert_model.embeddings(packed_query_ids).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
        "                document_ctx = torch.nn.functional.normalize(self.sample_cnn3(self.sample_projector(self.bert_model.embeddings(ids_packed).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
        "            elif self.sample_context == \"ck\":\n",
        "                query_ctx = torch.nn.functional.normalize(self.sample_cnn3((self.bert_model.embeddings(packed_query_ids).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
        "                document_ctx = torch.nn.functional.normalize(self.sample_cnn3((self.bert_model.embeddings(ids_packed).detach()).transpose(1,2)).transpose(1, 2),p=2,dim=-1)\n",
        "            else:\n",
        "                qe = self.tk_projector(self.bert_model.embeddings(packed_query_ids).detach())\n",
        "                de = self.tk_projector(self.bert_model.embeddings(ids_packed).detach())\n",
        "                query_ctx = self.tk_contextualizer(qe.transpose(1,0),src_key_padding_mask=~packed_query_mask.bool()).transpose(1,0)\n",
        "                document_ctx = self.tk_contextualizer(de.transpose(1,0),src_key_padding_mask=~mask_packed.bool()).transpose(1,0)\n",
        "        \n",
        "                query_ctx =   torch.nn.functional.normalize(query_ctx,p=2,dim=-1)\n",
        "                document_ctx= torch.nn.functional.normalize(document_ctx,p=2,dim=-1)\n",
        "\n",
        "            cosine_matrix = torch.bmm(query_ctx,document_ctx.transpose(-1, -2)).unsqueeze(-1)\n",
        "\n",
        "            kernel_activations = torch.exp(- torch.pow(cosine_matrix - self.mu, 2) / (2 * torch.pow(self.sigma, 2))) * mask_packed.unsqueeze(-1).unsqueeze(1)\n",
        "            kernel_res = torch.log(torch.clamp(torch.sum(kernel_activations, 2) * self.kernel_alpha_scaler, min=1e-4)) * packed_query_mask.unsqueeze(-1)\n",
        "            packed_patch_scores = self.sampling_binweights(torch.sum(kernel_res, 1))\n",
        "\n",
        "            \n",
        "            sampling_scores_per_doc = torch.zeros((total_chunks,1), dtype=packed_patch_scores.dtype, layout=packed_patch_scores.layout, device=packed_patch_scores.device)\n",
        "            sampling_scores_per_doc[packed_indices] = packed_patch_scores\n",
        "            sampling_scores_per_doc = sampling_scores_per_doc.reshape(batch_size,-1,)\n",
        "            sampling_scores_per_doc_orig = sampling_scores_per_doc.clone()\n",
        "            sampling_scores_per_doc[sampling_scores_per_doc == 0] = -9000\n",
        "\n",
        "            sampling_sorted = sampling_scores_per_doc.sort(descending=True)\n",
        "            sampled_indices = sampling_sorted.indices + torch.arange(0,sampling_scores_per_doc.shape[0]*sampling_scores_per_doc.shape[1],sampling_scores_per_doc.shape[1],device=sampling_scores_per_doc.device).unsqueeze(-1)\n",
        "\n",
        "            sampled_indices = sampled_indices[:,:self.sample_n]\n",
        "            sampled_indices_mask = torch.zeros_like(packed_indices).scatter(0, sampled_indices.reshape(-1), 1)\n",
        "\n",
        "            # pack indices\n",
        "\n",
        "            packed_indices = sampled_indices_mask * packed_indices\n",
        "    \n",
        "            packed_query_ids = query[\"input_ids\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"input_ids\"].shape[1])[packed_indices]\n",
        "            packed_query_mask = query[\"attention_mask\"].unsqueeze(1).expand(-1,chunk_pieces,-1).reshape(-1,query[\"attention_mask\"].shape[1])[packed_indices]\n",
        "\n",
        "            ids_packed = chunked_ids_unrolled[packed_indices]\n",
        "            mask_packed = (ids_packed != self.padding_idx)\n",
        "\n",
        "        #\n",
        "        # expensive bert scores\n",
        "        #\n",
        "        \n",
        "        bert_vecs = self.forward_representation(torch.cat([packed_query_ids,ids_packed],dim=1),torch.cat([packed_query_mask,mask_packed],dim=1))\n",
        "        packed_patch_scores = self._classification_layer(bert_vecs) \n",
        "\n",
        "        scores_per_doc = torch.zeros((total_chunks,1), dtype=packed_patch_scores.dtype, layout=packed_patch_scores.layout, device=packed_patch_scores.device)\n",
        "        scores_per_doc[packed_indices] = packed_patch_scores\n",
        "        scores_per_doc = scores_per_doc.reshape(batch_size,-1,)\n",
        "        scores_per_doc_orig = scores_per_doc.clone()\n",
        "        scores_per_doc_orig_sorter = scores_per_doc.clone()\n",
        "\n",
        "        if self.sample_n > -1:\n",
        "            scores_per_doc = scores_per_doc * sampled_indices_mask.view(batch_size,-1)\n",
        "        \n",
        "        #\n",
        "        # aggregate bert scores\n",
        "        #\n",
        "\n",
        "        if scores_per_doc.shape[1] < self.top_k_chunks:\n",
        "            scores_per_doc = nn.functional.pad(scores_per_doc,(0, self.top_k_chunks - scores_per_doc.shape[1]))\n",
        "\n",
        "        scores_per_doc[scores_per_doc == 0] = -9000\n",
        "        scores_per_doc_orig_sorter[scores_per_doc_orig_sorter == 0] = -9000\n",
        "        score = torch.sort(scores_per_doc,descending=True,dim=-1).values\n",
        "        score[score <= -8900] = 0\n",
        "\n",
        "        score = (score[:,:self.top_k_chunks] * self.top_k_scoring).sum(dim=1)\n",
        "\n",
        "        if self.sample_n == -1:\n",
        "            if output_secondary_output:\n",
        "                return score,{\n",
        "                    \"packed_indices\": orig_packed_indices.view(batch_size,-1),\n",
        "                    \"bert_scores\":scores_per_doc_orig\n",
        "                }\n",
        "            else:\n",
        "                return score,scores_per_doc_orig    \n",
        "        else:\n",
        "            if output_secondary_output:\n",
        "                return score,scores_per_doc_orig,{\n",
        "                    \"score\": score,\n",
        "                    \"packed_indices\": orig_packed_indices.view(batch_size,-1),\n",
        "                    \"sampling_scores\":sampling_scores_per_doc_orig,\n",
        "                    \"bert_scores\":scores_per_doc_orig\n",
        "                }\n",
        "\n",
        "            return score\n",
        "\n",
        "    def forward_representation(self, ids,mask,type_ids=None) -> Dict[str, torch.Tensor]:\n",
        "        \n",
        "        if self.bert_model.base_model_prefix == 'distilbert': # diff input / output \n",
        "            pooled = self.bert_model(input_ids=ids,\n",
        "                                     attention_mask=mask)[0][:,0,:]\n",
        "        elif self.bert_model.base_model_prefix == 'longformer':\n",
        "            _, pooled = self.bert_model(input_ids=ids,\n",
        "                                        attention_mask=mask.long(),\n",
        "                                        global_attention_mask = ((1-ids)*mask).long())\n",
        "        elif self.bert_model.base_model_prefix == 'roberta': # no token type ids\n",
        "            _, pooled = self.bert_model(input_ids=ids,\n",
        "                                        attention_mask=mask)\n",
        "        else:\n",
        "            _, pooled = self.bert_model(input_ids=ids,\n",
        "                                        token_type_ids=type_ids,\n",
        "                                        attention_mask=mask)\n",
        "\n",
        "        return pooled\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # honestly not sure if that is the best way to go, but it works :)\n",
        "idcm_model = IDCM_InferenceOnly.from_pretrained(pre_trained_model_name)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOGT8YQQX1Ot"
      },
      "source": [
        "Now we are ready to use the model to encode two sample passages and a query:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzt9Ix9UYMLy",
        "outputId": "529e338e-b4e7-4251-cf9b-4363ac8a3ed8"
      },
      "source": [
        "# our relevant example\n",
        "passage1_input = tokenizer(\"We are very happy to show you the ü§ó Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP üî•.The easiest way to use a pretrained model on a given task is to use pipeline(). ü§ó Transformers provides the following tasks out of the box: Sentiment analysis: is a text positive or negative? Text generation (in English): provide a prompt and the model will generate what follows. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.) Question answering: provide the model with some context and a question, extract the answer from the context. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks. Summarization: generate a summary of a long text. Translation: translate a text in another language. Feature extraction: return a tensor representation of the text. Let‚Äôs see how this work for sentiment analysis (the other tasks are all covered in the task summary): We are very happy to show you the ü§ó Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP üî•.The easiest way to use a pretrained model on a given task is to use pipeline(). ü§ó Transformers provides the following tasks out of the box: Sentiment analysis: is a text positive or negative? Text generation (in English): provide a prompt and the model will generate what follows. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.) Question answering: provide the model with some context and a question, extract the answer from the context. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks. Summarization: generate a summary of a long text. Translation: translate a text in another language. Feature extraction: return a tensor representation of the text. Let‚Äôs see how this work for sentiment analysis (the other tasks are all covered in the task summary): We are very happy to show you the ü§ó Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP üî•.The easiest way to use a pretrained model on a given task is to use pipeline(). ü§ó Transformers provides the following tasks out of the box: Sentiment analysis: is a text positive or negative? Text generation (in English): provide a prompt and the model will generate what follows. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.) Question answering: provide the model with some context and a question, extract the answer from the context. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks. Summarization: generate a summary of a long text. Translation: translate a text in another language. Feature extraction: return a tensor representation of the text. Let‚Äôs see how this work for sentiment analysis (the other tasks are all covered in the task summary): We are very happy to show you the ü§ó Transformers library for pre-trained language models. We are helping the community work together towards the goal of advancing NLP üî•.The easiest way to use a pretrained model on a given task is to use pipeline(). ü§ó Transformers provides the following tasks out of the box: Sentiment analysis: is a text positive or negative? Text generation (in English): provide a prompt and the model will generate what follows. Name entity recognition (NER): in an input sentence, label each word with the entity it represents (person, place, etc.) Question answering: provide the model with some context and a question, extract the answer from the context. Filling masked text: given a text with masked words (e.g., replaced by [MASK]), fill the blanks. Summarization: generate a summary of a long text. Translation: translate a text in another language. Feature extraction: return a tensor representation of the text. Let‚Äôs see how this work for sentiment analysis (the other tasks are all covered in the task summary):\",return_tensors=\"pt\",max_length=2000,truncation=True)\n",
        "# a non-relevant example\n",
        "passage2_input = tokenizer(\"Most machine learning workflows involve working with data, creating models, optimizing model parameters, and saving the trained models. This tutorial introduces you to a complete ML workflow implemented in PyTorch, with links to learn more about each of these concepts. We‚Äôll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, or Ankle boot. This tutorial assumes a basic familiarity with Python and Deep Learning concepts. Running the Tutorial CodeYou can run this tutorial in a couple of ways: In the cloud: This is the easiest way to get started! Each section has a ‚ÄúRun in Microsoft Learn‚Äù link at the top, which opens an integrated notebook in Microsoft Learn with the code in a fully-hosted environment. Locally: This option requires you to setup PyTorch and TorchVision first on your local machine (installation instructions). Download the notebook or copy the code into your favorite IDE.\",return_tensors=\"pt\",max_length=2000)\n",
        "# the user query -> which should give us a better score for the first passage\n",
        "query_input = tokenizer(\"what is the transformers library\",return_tensors=\"pt\",max_length=30,truncation=True)\n",
        "\n",
        "print(\"Passage 1 Tokenized:\",len(passage1_input[\"input_ids\"][0]),tokenizer.convert_ids_to_tokens(passage1_input[\"input_ids\"][0]),\"\\n\")\n",
        "print(\"Passage 2 Tokenized:\",len(passage2_input[\"input_ids\"][0]),tokenizer.convert_ids_to_tokens(passage2_input[\"input_ids\"][0]),\"\\n\")\n",
        "print(\"Query Tokenized:\",tokenizer.convert_ids_to_tokens(query_input[\"input_ids\"][0]))\n",
        "\n",
        "score1 = idcm_model(query_input, passage1_input).squeeze(0)\n",
        "score2 = idcm_model(query_input, passage2_input).squeeze(0)\n",
        "\n",
        "print(\"---\")\n",
        "print(\"score1:\",score1)\n",
        "print(\"score2\",score2)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passage 1 Tokenized: 898 ['[CLS]', 'we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library', 'for', 'pre', '-', 'trained', 'language', 'models', '.', 'we', 'are', 'helping', 'the', 'community', 'work', 'together', 'towards', 'the', 'goal', 'of', 'advancing', 'nl', '##p', '[UNK]', '.', 'the', 'easiest', 'way', 'to', 'use', 'a', 'pre', '##train', '##ed', 'model', 'on', 'a', 'given', 'task', 'is', 'to', 'use', 'pipeline', '(', ')', '.', '[UNK]', 'transformers', 'provides', 'the', 'following', 'tasks', 'out', 'of', 'the', 'box', ':', 'sentiment', 'analysis', ':', 'is', 'a', 'text', 'positive', 'or', 'negative', '?', 'text', 'generation', '(', 'in', 'english', ')', ':', 'provide', 'a', 'prompt', 'and', 'the', 'model', 'will', 'generate', 'what', 'follows', '.', 'name', 'entity', 'recognition', '(', 'ne', '##r', ')', ':', 'in', 'an', 'input', 'sentence', ',', 'label', 'each', 'word', 'with', 'the', 'entity', 'it', 'represents', '(', 'person', ',', 'place', ',', 'etc', '.', ')', 'question', 'answering', ':', 'provide', 'the', 'model', 'with', 'some', 'context', 'and', 'a', 'question', ',', 'extract', 'the', 'answer', 'from', 'the', 'context', '.', 'filling', 'masked', 'text', ':', 'given', 'a', 'text', 'with', 'masked', 'words', '(', 'e', '.', 'g', '.', ',', 'replaced', 'by', '[MASK]', ')', ',', 'fill', 'the', 'blank', '##s', '.', 'sum', '##mar', '##ization', ':', 'generate', 'a', 'summary', 'of', 'a', 'long', 'text', '.', 'translation', ':', 'translate', 'a', 'text', 'in', 'another', 'language', '.', 'feature', 'extraction', ':', 'return', 'a', 'tensor', 'representation', 'of', 'the', 'text', '.', 'let', '‚Äô', 's', 'see', 'how', 'this', 'work', 'for', 'sentiment', 'analysis', '(', 'the', 'other', 'tasks', 'are', 'all', 'covered', 'in', 'the', 'task', 'summary', ')', ':', 'we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library', 'for', 'pre', '-', 'trained', 'language', 'models', '.', 'we', 'are', 'helping', 'the', 'community', 'work', 'together', 'towards', 'the', 'goal', 'of', 'advancing', 'nl', '##p', '[UNK]', '.', 'the', 'easiest', 'way', 'to', 'use', 'a', 'pre', '##train', '##ed', 'model', 'on', 'a', 'given', 'task', 'is', 'to', 'use', 'pipeline', '(', ')', '.', '[UNK]', 'transformers', 'provides', 'the', 'following', 'tasks', 'out', 'of', 'the', 'box', ':', 'sentiment', 'analysis', ':', 'is', 'a', 'text', 'positive', 'or', 'negative', '?', 'text', 'generation', '(', 'in', 'english', ')', ':', 'provide', 'a', 'prompt', 'and', 'the', 'model', 'will', 'generate', 'what', 'follows', '.', 'name', 'entity', 'recognition', '(', 'ne', '##r', ')', ':', 'in', 'an', 'input', 'sentence', ',', 'label', 'each', 'word', 'with', 'the', 'entity', 'it', 'represents', '(', 'person', ',', 'place', ',', 'etc', '.', ')', 'question', 'answering', ':', 'provide', 'the', 'model', 'with', 'some', 'context', 'and', 'a', 'question', ',', 'extract', 'the', 'answer', 'from', 'the', 'context', '.', 'filling', 'masked', 'text', ':', 'given', 'a', 'text', 'with', 'masked', 'words', '(', 'e', '.', 'g', '.', ',', 'replaced', 'by', '[MASK]', ')', ',', 'fill', 'the', 'blank', '##s', '.', 'sum', '##mar', '##ization', ':', 'generate', 'a', 'summary', 'of', 'a', 'long', 'text', '.', 'translation', ':', 'translate', 'a', 'text', 'in', 'another', 'language', '.', 'feature', 'extraction', ':', 'return', 'a', 'tensor', 'representation', 'of', 'the', 'text', '.', 'let', '‚Äô', 's', 'see', 'how', 'this', 'work', 'for', 'sentiment', 'analysis', '(', 'the', 'other', 'tasks', 'are', 'all', 'covered', 'in', 'the', 'task', 'summary', ')', ':', 'we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library', 'for', 'pre', '-', 'trained', 'language', 'models', '.', 'we', 'are', 'helping', 'the', 'community', 'work', 'together', 'towards', 'the', 'goal', 'of', 'advancing', 'nl', '##p', '[UNK]', '.', 'the', 'easiest', 'way', 'to', 'use', 'a', 'pre', '##train', '##ed', 'model', 'on', 'a', 'given', 'task', 'is', 'to', 'use', 'pipeline', '(', ')', '.', '[UNK]', 'transformers', 'provides', 'the', 'following', 'tasks', 'out', 'of', 'the', 'box', ':', 'sentiment', 'analysis', ':', 'is', 'a', 'text', 'positive', 'or', 'negative', '?', 'text', 'generation', '(', 'in', 'english', ')', ':', 'provide', 'a', 'prompt', 'and', 'the', 'model', 'will', 'generate', 'what', 'follows', '.', 'name', 'entity', 'recognition', '(', 'ne', '##r', ')', ':', 'in', 'an', 'input', 'sentence', ',', 'label', 'each', 'word', 'with', 'the', 'entity', 'it', 'represents', '(', 'person', ',', 'place', ',', 'etc', '.', ')', 'question', 'answering', ':', 'provide', 'the', 'model', 'with', 'some', 'context', 'and', 'a', 'question', ',', 'extract', 'the', 'answer', 'from', 'the', 'context', '.', 'filling', 'masked', 'text', ':', 'given', 'a', 'text', 'with', 'masked', 'words', '(', 'e', '.', 'g', '.', ',', 'replaced', 'by', '[MASK]', ')', ',', 'fill', 'the', 'blank', '##s', '.', 'sum', '##mar', '##ization', ':', 'generate', 'a', 'summary', 'of', 'a', 'long', 'text', '.', 'translation', ':', 'translate', 'a', 'text', 'in', 'another', 'language', '.', 'feature', 'extraction', ':', 'return', 'a', 'tensor', 'representation', 'of', 'the', 'text', '.', 'let', '‚Äô', 's', 'see', 'how', 'this', 'work', 'for', 'sentiment', 'analysis', '(', 'the', 'other', 'tasks', 'are', 'all', 'covered', 'in', 'the', 'task', 'summary', ')', ':', 'we', 'are', 'very', 'happy', 'to', 'show', 'you', 'the', '[UNK]', 'transformers', 'library', 'for', 'pre', '-', 'trained', 'language', 'models', '.', 'we', 'are', 'helping', 'the', 'community', 'work', 'together', 'towards', 'the', 'goal', 'of', 'advancing', 'nl', '##p', '[UNK]', '.', 'the', 'easiest', 'way', 'to', 'use', 'a', 'pre', '##train', '##ed', 'model', 'on', 'a', 'given', 'task', 'is', 'to', 'use', 'pipeline', '(', ')', '.', '[UNK]', 'transformers', 'provides', 'the', 'following', 'tasks', 'out', 'of', 'the', 'box', ':', 'sentiment', 'analysis', ':', 'is', 'a', 'text', 'positive', 'or', 'negative', '?', 'text', 'generation', '(', 'in', 'english', ')', ':', 'provide', 'a', 'prompt', 'and', 'the', 'model', 'will', 'generate', 'what', 'follows', '.', 'name', 'entity', 'recognition', '(', 'ne', '##r', ')', ':', 'in', 'an', 'input', 'sentence', ',', 'label', 'each', 'word', 'with', 'the', 'entity', 'it', 'represents', '(', 'person', ',', 'place', ',', 'etc', '.', ')', 'question', 'answering', ':', 'provide', 'the', 'model', 'with', 'some', 'context', 'and', 'a', 'question', ',', 'extract', 'the', 'answer', 'from', 'the', 'context', '.', 'filling', 'masked', 'text', ':', 'given', 'a', 'text', 'with', 'masked', 'words', '(', 'e', '.', 'g', '.', ',', 'replaced', 'by', '[MASK]', ')', ',', 'fill', 'the', 'blank', '##s', '.', 'sum', '##mar', '##ization', ':', 'generate', 'a', 'summary', 'of', 'a', 'long', 'text', '.', 'translation', ':', 'translate', 'a', 'text', 'in', 'another', 'language', '.', 'feature', 'extraction', ':', 'return', 'a', 'tensor', 'representation', 'of', 'the', 'text', '.', 'let', '‚Äô', 's', 'see', 'how', 'this', 'work', 'for', 'sentiment', 'analysis', '(', 'the', 'other', 'tasks', 'are', 'all', 'covered', 'in', 'the', 'task', 'summary', ')', ':', '[SEP]'] \n",
            "\n",
            "Passage 2 Tokenized: 233 ['[CLS]', 'most', 'machine', 'learning', 'work', '##flow', '##s', 'involve', 'working', 'with', 'data', ',', 'creating', 'models', ',', 'opt', '##imi', '##zing', 'model', 'parameters', ',', 'and', 'saving', 'the', 'trained', 'models', '.', 'this', 'tutor', '##ial', 'introduces', 'you', 'to', 'a', 'complete', 'ml', 'work', '##flow', 'implemented', 'in', 'p', '##yt', '##or', '##ch', ',', 'with', 'links', 'to', 'learn', 'more', 'about', 'each', 'of', 'these', 'concepts', '.', 'we', '‚Äô', 'll', 'use', 'the', 'fashion', '##m', '##nist', 'data', '##set', 'to', 'train', 'a', 'neural', 'network', 'that', 'predict', '##s', 'if', 'an', 'input', 'image', 'belongs', 'to', 'one', 'of', 'the', 'following', 'classes', ':', 't', '-', 'shirt', '/', 'top', ',', 'tr', '##ouse', '##r', ',', 'pull', '##over', ',', 'dress', ',', 'coat', ',', 'sand', '##al', ',', 'shirt', ',', 'sneak', '##er', ',', 'bag', ',', 'or', 'ankle', 'boot', '.', 'this', 'tutor', '##ial', 'assumes', 'a', 'basic', 'familiarity', 'with', 'python', 'and', 'deep', 'learning', 'concepts', '.', 'running', 'the', 'tutor', '##ial', 'code', '##you', 'can', 'run', 'this', 'tutor', '##ial', 'in', 'a', 'couple', 'of', 'ways', ':', 'in', 'the', 'cloud', ':', 'this', 'is', 'the', 'easiest', 'way', 'to', 'get', 'started', '!', 'each', 'section', 'has', 'a', '‚Äú', 'run', 'in', 'microsoft', 'learn', '‚Äù', 'link', 'at', 'the', 'top', ',', 'which', 'opens', 'an', 'integrated', 'notebook', 'in', 'microsoft', 'learn', 'with', 'the', 'code', 'in', 'a', 'fully', '-', 'hosted', 'environment', '.', 'locally', ':', 'this', 'option', 'requires', 'you', 'to', 'setup', 'p', '##yt', '##or', '##ch', 'and', 'torch', '##vision', 'first', 'on', 'your', 'local', 'machine', '(', 'installation', 'instructions', ')', '.', 'download', 'the', 'notebook', 'or', 'copy', 'the', 'code', 'into', 'your', 'favorite', 'id', '##e', '.', '[SEP]'] \n",
            "\n",
            "Query Tokenized: ['[CLS]', 'what', 'is', 'the', 'transformers', 'library', '[SEP]']\n",
            "---\n",
            "score1: tensor(-7.2648, grad_fn=<SqueezeBackward1>)\n",
            "score2 tensor(-13.7012, grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1bY5qB9b-AI"
      },
      "source": [
        "As we see the model gives the first passage a higher score than the second - yeah!\n",
        "\n",
        "- If you use our model checkpoint please cite our work as:\n",
        "\n",
        "    ```\n",
        "@inproceedings{Hofstaetter2021_idcm,\n",
        " author = {Sebastian Hofst{\\\"a}tter and Bhaskar Mitra and Hamed Zamani and Nick Craswell and Allan Hanbury},\n",
        " title = {{Intra-Document Cascading: Learning to Select Passages for Neural Document Ranking}},\n",
        " booktitle = {Proc. of SIGIR},\n",
        " year = {2021},\n",
        "}\n",
        "    ```\n",
        "\n",
        "Thank You üòä If you have any questions feel free to reach out to Sebastian via mail (email in the paper). \n"
      ]
    }
  ]
}